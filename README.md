# normalising your inputs
# concatenate countVectorizer with word2vec


training loss should follwo the same pattern -> or training loss is decreasing and val loss is not then overfitting majorly


random seeding splitting- if same pattern then memorizing


# word2vec, gloVe

#batch size should be 2^n

#if learning rate is tooo high -> it could be there are spike in trainng loss curve 
#droupout and weight_decay
# playing with the hyperparmamters of the forward layer - output dimension of a forward layer


